import os
import fitz  # PyMuPDF
import re
import numpy as np
import faiss
import openai
import nltk
from nltk.tokenize import sent_tokenize
from sentence_transformers import SentenceTransformer

nltk.download('punkt')

def extract_text_from_pdfs(pdf_paths):
    all_text = ""
    for pdf_path in pdf_paths:
        if not os.path.exists(pdf_path):
            print(f"Warning: PDF file '{pdf_path}' not found.")
            continue
        doc = fitz.open(pdf_path)
        all_text += "".join(page.get_text() for page in doc) + "\n"
    return all_text

def clean_text(text):
    text = re.sub(r'\s+', ' ', text)
    text = re.sub(r'[^\w\s.,;!?]', '', text)
    return text.strip()

def split_into_paragraphs(text, max_length=200):
    paragraphs = []
    current_paragraph = ""
    sentences = sent_tokenize(text)
    for sentence in sentences:
        if len(current_paragraph) + len(sentence) <= max_length:
            current_paragraph += sentence + " "
        else:
            paragraphs.append(current_paragraph.strip())
            current_paragraph = sentence + " "
    if current_paragraph:
        paragraphs.append(current_paragraph.strip())
    return paragraphs

def build_faiss_index(paragraphs, model):
    if not paragraphs:
        raise ValueError("Paragraph list is empty. Check the extracted text.")
    paragraph_embeddings = model.encode(paragraphs)
    dimension = paragraph_embeddings.shape[1]
    index = faiss.IndexFlatL2(dimension)
    index.add(paragraph_embeddings)
    return index, paragraph_embeddings

def retrieve_relevant_chunks(query, index, paragraphs, model, k=3):
    if index.ntotal == 0:
        raise ValueError("FAISS index is empty. Make sure embeddings are added.")
    query_embedding = model.encode([query])
    distances, indices = index.search(query_embedding, k)
    return [paragraphs[i] for i in indices[0]]

api_key = "APIKey"

# OpenAI istemcisi oluştur
client = openai.OpenAI(api_key=api_key)

# ChatGPT yanıt üretme fonksiyonu
def generate_response_with_chatgpt(query, context):
    try:
        response = client.chat.completions.create(
            model="gpt-4",  # veya "gpt-3.5-turbo"
            messages=[
                {"role": "system", "content": "You are a helpful assistant. Use the following context to answer the user's query."},
                {"role": "user", "content": f"Context: {context}\n\nQuery: {query}"}
            ]
        )
        return response.choices[0].message.content
    except openai.OpenAIError as e:
        print(f"OpenAI API Hatası: {e}")
        return "Üzgünüm, şu an yanıt veremiyorum."


if __name__ == "__main__":
    pdf_paths = ["fifa.pdf", "doncic.pdf"]
    pdf_text = extract_text_from_pdfs(pdf_paths)
    cleaned_text = clean_text(pdf_text)
    paragraphs = split_into_paragraphs(cleaned_text)
    model = SentenceTransformer('all-MiniLM-L6-v2')
    index, _ = build_faiss_index(paragraphs, model)
    
    if index.ntotal == 0:
        print("Warning: FAISS index is empty. The retrieval may not work properly.")
    
    query = "which country is the host for 2022 fifa world cup?"
    relevant_chunks = retrieve_relevant_chunks(query, index, paragraphs, model)
    context = "\n".join(relevant_chunks)
    response = generate_response_with_chatgpt(query, context)
    
    print("Retrieved Context:\n", context)
    print("\nGenerated Response:\n", response)
